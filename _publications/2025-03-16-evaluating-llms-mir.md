---
title: "Evaluating Large Language Models on the Spanish Medical Intern Resident (MIR) Examination 2024/2025"
collection: publications
category: manuscripts
permalink: /publication/2025-03-16-evaluating-llms-mir
excerpt: 'Comparative analysis of 22 generative AI models in clinical reasoning and performance across the MIR 2024/2025 medical exams.'
date: 2025-03-16
venue: 'arXiv'
paperurl: 'https://arxiv.org/abs/2503.00025'
bibtexurl: ''
citation: 'Carlos Luengo Vera et al. (2025). "Evaluating Large Language Models on the Spanish Medical Intern Resident (MIR) Examination 2024/2025." <i>arXiv</i>. DOI: 10.48550/arXiv.2503.00025'
tags:
  - Large Language Models
  - MIR Exam
  - Medical AI
  - Clinical Reasoning
  - Research
image: /images/post/2025/2025_mir_llms.png
---

This study presents a comparative evaluation of **22 large language models (LLMs)** on the Spanish Medical Intern Resident (MIR) examinations for 2024 and 2025, with a focus on clinical reasoning, domain-specific expertise, and multimodal processing capabilities.

The MIR exam, consisting of 210 multiple choice questions (some requiring image interpretation), serves as a stringent benchmark for assessing both factual recall and complex clinical problem-solving skills. The investigation encompasses general-purpose models such as **GPT-4, Claude, LLaMA, and Gemini**, as well as specialized fine-tuned systems like **Miri Pro**, which leverages proprietary Spanish healthcare data to excel in medical contexts.

Recent market entries like **Deepseek** and **Grok** have further enriched the evaluation landscape, particularly for tasks that demand advanced visual and semantic analysis.

## Key Findings

- **Fine-tuned models** (e.g., Miri Pro) consistently achieve superior accuracy, especially in nuanced, domain-specific challenges.
- **General-purpose LLMs** perform robustly overall, but lag behind in Spain-specific public health scenarios and complex clinical reasoning.
- **Multimodal models** show superior performance on image-based questions.
- A modest performance decline between the two exam cycles appears attributable to the implementation of modified questions designed to mitigate reliance on memorization.
- **Ethical reasoning** remains a challenge for all AI systems evaluated.

## Implications

The results underscore the transformative potential of **domain-specific fine-tuning** and **multimodal integration** in advancing medical AI applications. They also highlight critical implications for the future integration of LLMs into medical education, training, and clinical decision-makingâ€”emphasizing the importance of balancing automated reasoning with ethical and context-aware judgment.

**Comments:** 26 pages, 1 table, 7 figures  
**Subjects:** Computation and Language (cs.CL); Artificial Intelligence (cs.AI)  
**Cite as:** [arXiv:2503.00025 [cs.CL]](https://arxiv.org/abs/2503.00025)

*If you are interested in the intersection of AI and medical education, this paper provides a comprehensive benchmark and valuable insights for both researchers and practitioners.* 